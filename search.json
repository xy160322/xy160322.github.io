[
  {
    "objectID": "Interactive-1.html",
    "href": "Interactive-1.html",
    "title": "",
    "section": "",
    "text": "已连接到 sklearn-env (Python 3.13.1)\n\n# -*- coding: utf-8 -*-\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf=pd.read_csv(\"house.csv\",header=0)\nx=list(df[\"area\"])\ny=list(df[\"price\"])\nplt.figure(figsize=(15,6))\nplt.xlabel('area')\nplt.ylabel('price')\nplt.scatter(x,y)\nplt.show()\n\n\n\n\n\n\n\n\n已连接到 sklearn-env (Python 3.13.1)\n\n# %pip install ipywidgets\n# %matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker, cm\nimport seaborn as sns\nfrom ipywidgets import *\nimport math\n\nsns.set_context('paper', font_scale=2)\nsns.set_style('ticks')\ndef f_2d(x1, x2):\n    '''original function to minimize'''\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n\ndef f_grad(x1, x2):\n    '''the gradient dfdx1 and dfdx2'''\n    dfdx1 = 0.2 * x1\n    dfdx2 = 4 * x2\n    return dfdx1, dfdx2\n\ndef train_2d(trainer, lr):\n    \"\"\"Train a 2d object function with a customized trainer\"\"\"\n    x1, x2 = -5, -2\n    s_x1, s_x2 = 0, 0\n    res = [(x1, x2)]\n    for i in range(50):\n        x1, x2, s_x1, s_x2, lr = trainer(x1, x2, s_x1, s_x2, lr)\n        res.append((x1, x2))\n    return res\n\ndef plot_2d(res, figsize=(10, 6), title=None):\n    x1_, x2_ = zip(*res)\n    fig = plt.figure(figsize=figsize)\n    plt.plot([0], [0], 'r*', ms=15)\n    plt.text(0.0, 0.25, 'minimum', color='w')\n    plt.plot(x1_[0], x2_[0], 'ro', ms=10)\n    plt.text(x1_[0]+0.1, x2_[0]+0.2, 'start', color='w')\n    plt.plot(x1_, x2_, '-o', color='#ff7f0e')\n    \n    plt.plot(x1_[-1], x2_[-1], 'wo')\n    plt.text(x1_[-1], x2_[-1]-0.25, 'end', color='w')\n    \n    x1 = np.linspace(-5.5, 3, 50)\n    x2 = np.linspace(min(-3.0, min(x2_) - 1), max(3.0, max(x2_) + 1), 100)\n    x1, x2 = np.meshgrid(x1, x2)\n    plt.contourf(x1, x2, f_2d(x1, x2), cmap=cm.gnuplot)\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.title(title)\n    plt.show()\n\ndef sgd(x1, x2, s1, s2, lr):\n    dfdx1, dfdx2 = f_grad(x1, x2)\n    return (x1 - lr * dfdx1, x2 - lr * dfdx2, 0, 0, lr)\n\n@interact(lr=(0, 1, 0.001))\ndef visualize_gradient_descent(lr=0.05):\n    res = train_2d(sgd, lr)\n    plot_2d(res, title='SGD')\n\n\n\n\n\n#  tbl-cap: Astronomical object\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n# table = [[\"Sun\",\"696,000\",1.989e30],\n#          [\"Earth\",\"6,371\",5.972e24],\n#          [\"Moon\",\"1,737\",7.34e22],\n#          [\"Mars\",\"3,390\",6.39e23]]\ntable=l\nMarkdown(tabulate(\n  table, \n  headers=[\"Points\",\"x\", \"y\"]\n))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nFile /Users/xy/Documents/ML讲义/机器学习讲义/Lec1/linear_regression.qmd:8\n      3 from tabulate import tabulate\n      4 # table = [[\"Sun\",\"696,000\",1.989e30],\n      5 #          [\"Earth\",\"6,371\",5.972e24],\n      6 #          [\"Moon\",\"1,737\",7.34e22],\n      7 #          [\"Mars\",\"3,390\",6.39e23]]\n----&gt; 8 table=l\n      9 Markdown(tabulate(\n     10   table, \n     11   headers=[\"Points\",\"x\", \"y\"]\n     12 ))\n\nNameError: name 'l' is not defined\n\n\n\n\n#  tbl-cap: Astronomical object\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n# table = [[\"Sun\",\"696,000\",1.989e30],\n#          [\"Earth\",\"6,371\",5.972e24],\n#          [\"Moon\",\"1,737\",7.34e22],\n#          [\"Mars\",\"3,390\",6.39e23]]\ntable=l\nMarkdown(tabulate(\n  table, \n  headers=[\"Points\",\"x\", \"y\"]\n))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nFile /Users/xy/Documents/ML讲义/机器学习讲义/Lec1/linear_regression.qmd:8\n      3 from tabulate import tabulate\n      4 # table = [[\"Sun\",\"696,000\",1.989e30],\n      5 #          [\"Earth\",\"6,371\",5.972e24],\n      6 #          [\"Moon\",\"1,737\",7.34e22],\n      7 #          [\"Mars\",\"3,390\",6.39e23]]\n----&gt; 8 table=l\n      9 Markdown(tabulate(\n     10   table, \n     11   headers=[\"Points\",\"x\", \"y\"]\n     12 ))\n\nNameError: name 'l' is not defined\n\n\n\n\n#  tbl-cap: Astronomical object\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n# table = [[\"Sun\",\"696,000\",1.989e30],\n#          [\"Earth\",\"6,371\",5.972e24],\n#          [\"Moon\",\"1,737\",7.34e22],\n#          [\"Mars\",\"3,390\",6.39e23]]\ntable=l\nMarkdown(tabulate(\n  table, \n  headers=[\"Points\",\"x\", \"y\"]\n))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nFile /Users/xy/Documents/ML讲义/机器学习讲义/Lec1/linear_regression.qmd:8\n      3 from tabulate import tabulate\n      4 # table = [[\"Sun\",\"696,000\",1.989e30],\n      5 #          [\"Earth\",\"6,371\",5.972e24],\n      6 #          [\"Moon\",\"1,737\",7.34e22],\n      7 #          [\"Mars\",\"3,390\",6.39e23]]\n----&gt; 8 table=l\n      9 Markdown(tabulate(\n     10   table, \n     11   headers=[\"Points\",\"x\", \"y\"]\n     12 ))\n\nNameError: name 'l' is not defined"
  },
  {
    "objectID": "linear_regression.html#例北京某小区房价数据",
    "href": "linear_regression.html#例北京某小区房价数据",
    "title": "0x01 线性回归",
    "section": "例：北京某小区房价数据",
    "text": "例：北京某小区房价数据\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narea\nprice\n\n\n\n\n0\n62.9\n680\n\n\n1\n44.3\n485\n\n\n2\n57.4\n555\n\n\n3\n62.4\n620\n\n\n4\n57.4\n585\n\n\n5\n…\n…\n\n\n\n\n\n\n\n行、列\n特征（属性）\n标签"
  },
  {
    "objectID": "linear_regression.html#问题与假设",
    "href": "linear_regression.html#问题与假设",
    "title": "0x01 线性回归",
    "section": "问题与假设",
    "text": "问题与假设\n\n问题：给定一组住房成交数据，每条数据有一个属性（面积 \\(area\\) ），一个标签（价格 \\(price\\) ），建立模型，能够实现给定一个新挂牌的房子的面积，预测它的成交价\n假设：所有\\((area,price)\\)点在同一条直线附近"
  },
  {
    "objectID": "linear_regression.html#建模",
    "href": "linear_regression.html#建模",
    "title": "0x01 线性回归",
    "section": "建模",
    "text": "建模\n\n训练集 \\(T\\)：采集到的样本数据\n\n一个训练数据记为 \\((x,y)\\)，第 \\(i\\) 个训练数据记为 \\((x_i,y_i)\\)\n训练集大小：\\(m\\)\n\n\\(T=\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}=\\{(x_i,y_i)\\}\\)\n\n决策函数假设：\\(f(x)=kx+\\)，属性 \\(x\\) 相应的预测结果 \\(f(x)\\)\n损失函数: \\(L(k,b;T)\\)，在已知训练集 \\(T\\) 的情况下关于决策函数参数 \\(k,b\\) 的函数，用于量化参数的效果，即在训练集上预测值与真实值的差距，一般越小越好。\n\n\\(L(k,b;T)=\\frac{1}{m}((f(x_1)-y_1)^2+(f(x_2)-y_2)^2+...+(f(x_m)-y_m)^2)=\\frac{1}{m}\\sum\\limits_{i=1}^m(f(x_i)-y_i)^2\\)\n\n学习问题（最优化问题）：选择某种算法，如导数法、配方法、梯度下降法等，求解使得 \\(L(k,b;T)\\) 取最小值时的参数 \\(k,b\\)"
  },
  {
    "objectID": "linear_regression.html#建模-1",
    "href": "linear_regression.html#建模-1",
    "title": "0x01 线性回归",
    "section": "建模",
    "text": "建模"
  },
  {
    "objectID": "linear_regression.html#练习1",
    "href": "linear_regression.html#练习1",
    "title": "0x01 线性回归",
    "section": "练习1",
    "text": "练习1\n\n给定训练集 \\(T=\\{(1,2),(2,3),(3,4),(4,2)\\}\\)， \\(m=4\\)\n决策函数：假设 \\(f(x)=kx\\)\n写出损失函数，并求最优参数 \\(k\\)"
  },
  {
    "objectID": "linear_regression.html#切线与导数",
    "href": "linear_regression.html#切线与导数",
    "title": "0x01 线性回归",
    "section": "切线与导数",
    "text": "切线与导数\n\n割线：记 \\(A\\) 为图像上一个定点，\\(B\\) 为图像上 \\(A\\) 附近的点，称直线 \\(AB\\) 为图像的割线\n切线：当 \\(B\\) 无限接近 \\(A\\) 时，割线 \\(AB\\) 无限接近经过 \\(A\\) 的一条直线 \\(l\\)，称 \\(l\\) 为图在点 \\(A\\) 处的切线。切线反映了切点附近函数值的变化趋势\n导数：切线的斜率"
  },
  {
    "objectID": "linear_regression.html#例yfrac18x2-在点-a20.5-处的切线",
    "href": "linear_regression.html#例yfrac18x2-在点-a20.5-处的切线",
    "title": "0x01 线性回归",
    "section": "例：\\(y=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线",
    "text": "例：\\(y=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线"
  },
  {
    "objectID": "linear_regression.html#例fxfrac18x2-在点-a20.5-处的切线斜率",
    "href": "linear_regression.html#例fxfrac18x2-在点-a20.5-处的切线斜率",
    "title": "0x01 线性回归",
    "section": "例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率",
    "text": "例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率\n\n方法一：记 \\(B\\) 点坐标为 \\(B(2+\\Delta x,f(2+\\Delta x))\\)，然后计算直线 \\(AB\\) 的斜率：\\(\\frac{f(2+\\Delta x)-0.5}{2+\\Delta x-2}\\)，给 \\(\\Delta x\\) 一个非常小的值，如 \\(0.00001\\)，得到切线斜率 \\(f'(2)\\) 的近似值，如 \\(0.50000125\\)，如果觉得不满意，就把 \\(\\Delta x\\) 再小一些……\n\n\n\n0.500001250003379\n\n\n\n方法二：对 \\(\\frac{f(2+\\Delta x)-0.5}{2+\\Delta x-2}\\) 继续进行推导，\\(\\frac{f(2+\\Delta x)-0.5}{2+\\Delta x-2}=\\frac{1}{2}+\\frac{1}{8}\\Delta x\\) ，当 \\(\\Delta x\\rightarrow 0\\) 时，\\(\\frac{f(2+\\Delta x)-0.5}{2+\\Delta x-2}\\rightarrow \\frac{1}{2}\\)，可以认为 \\(f'(2)=\\frac{1}{2}\\)\n事实上，\\(f(x)=\\frac{1}{8}x^2\\) 的过任意点 \\(A(x,f(x))\\) 的切线斜率 \\(f'(x)\\) 都可以按如上方法推导\n事实上，\\(f(x)=ax^2+bx+c\\) 的过点 \\(A(x,f(x))\\) 的切线斜率 \\(f'(x)\\) 也可按如上方法推导"
  },
  {
    "objectID": "linear_regression.html#练习2",
    "href": "linear_regression.html#练习2",
    "title": "0x01 线性回归",
    "section": "练习2",
    "text": "练习2\n\n推导 \\(f(x)=2x^2+3x-4\\) 在任意点 \\(A(x,f(x))\\) 的导数\n推导 \\(f(x)=ax^2+bx+c\\) 在任意点 \\(A(x,f(x))\\) 的导数，其中 \\(a,b,c\\) 为参数\n答案：\n\n\\(f'(x)=4x+3\\)\n\\(f'(x)=2ax+b\\)"
  },
  {
    "objectID": "linear_regression.html#切线斜率与最值的关系",
    "href": "linear_regression.html#切线斜率与最值的关系",
    "title": "0x01 线性回归",
    "section": "切线斜率与最值的关系",
    "text": "切线斜率与最值的关系"
  },
  {
    "objectID": "linear_regression.html#梯度下降法",
    "href": "linear_regression.html#梯度下降法",
    "title": "0x01 线性回归",
    "section": "梯度下降法",
    "text": "梯度下降法\n\n解析法（配方法）和导数法有局限性，有时损失函数\\(L\\)不容易直接求得最值点，或者\\(L'\\)不容易求得零点\n梯度下降法：从某一个选定的点出发，沿着梯度（导数）方向移动点，不断接近极值点的数值算法（有误差，但可控）。它是机器学习甚至更广泛领域常见的优化方法。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoints\nx\ny\n\n\n\n\nP1\n-2\n65.5\n\n\nP2\n-0.24\n11.292\n\n\nP3\n0.464\n2.61872\n\n\nP4\n0.7456\n1.231\n\n\nP5\n0.85824\n1.00896\n\n\nP6\n0.903296\n0.973433\n\n\nP7\n0.921318\n0.967749"
  },
  {
    "objectID": "linear_regression.html#梯度下降法-1",
    "href": "linear_regression.html#梯度下降法-1",
    "title": "0x01 线性回归",
    "section": "梯度下降法",
    "text": "梯度下降法\n\n考虑从一个点 \\(x_0\\) 开始逐步向最值点移动，最终能够到达（或无限接近）最值点\n\n选哪个点作为起点 \\(x_0\\) ？任意点？\n每次步长如何确定？\\[ x_{k+1}=x_k-\\alpha f'(x_k) \\]\n算法为什么是正确的？\n\n秘诀：切线指方向，导数定步长\n见house.xlsx演示"
  },
  {
    "objectID": "linear_regression.html#梯度下降法收敛性证明",
    "href": "linear_regression.html#梯度下降法收敛性证明",
    "title": "0x01 线性回归",
    "section": "梯度下降法收敛性证明",
    "text": "梯度下降法收敛性证明\n以 \\(f(x)=x^2\\) 为例，任选 \\(x_0\\) 为起点，更新规则 \\(x_{k+1}=x_k-\\alpha f'(x_k)(0\\leqslant \\alpha&lt;\\frac 1 2)\\)，随着\\(k\\)的增大，\\(f(x_k)\\rightarrow f(x^*)\\)，其中 \\(x^*\\) 为 \\(f(x)\\) 取得最小值的点\n\n由梯度下降更新公式：\\(x_{k+1}=x_k-\\alpha f'(x_k) =x_k-2\\alpha  x_k=x_k (1-2 \\alpha )\\)\n\\(x_k =x_{k-1}(1-2\\alpha)=x_{k-2}(1-2\\alpha)^2=x_{k-3}(1-2\\alpha)^3=...=x_{0}(1-2\\alpha)^k\\)\n由 \\(f(x)\\)的凸性，\\(\\forall a,b\\)，\\(f(b)\\geq f(a)+f'(a)(b-a)\\)（可将 \\(f(x)=x^2\\) 代入验证），取 \\(b=x^{*},a=x_k\\)，则有 \\[\\begin{align}\nf(x_k)- f(x^{*})&\\leqslant f'(x_k)(x_k-x^{*})\\\\\n&=2x_k(x_k-x^*)\\\\\n&=2\\left((x_k-\\frac{1}{2} x^*)^2-\\frac{1}{4} (x^*)^2\\right)\n\\end{align}\n\\]\n随着 \\(k\\) 的增大，由于 \\(0\\leqslant (1-2\\alpha)&lt;1\\)，所以当 \\(x_k\\rightarrow 0\\) 时，\\(2\\left((x_k-\\frac{1}{2} x^*)^2-\\frac{1}{4} (x^*)^2\\right)\\rightarrow 0\\)，而 \\(f(x_k)-f(x^*) \\geqslant 0\\)，所以 \\(f(x_k)\\rightarrow f(x^*)\\)"
  },
  {
    "objectID": "linear_regression.html#练习3",
    "href": "linear_regression.html#练习3",
    "title": "0x01 线性回归",
    "section": "练习3",
    "text": "练习3\n使用梯度下降方法计算 \\(f(x)=2x^2+3x-4\\) 的最值，任选 \\(\\alpha\\) 和 \\(x_0\\)，并与配方法和导数法得到的结果进行比较"
  },
  {
    "objectID": "linear_regression.html#思考题",
    "href": "linear_regression.html#思考题",
    "title": "0x01 线性回归",
    "section": "思考题",
    "text": "思考题\n以 \\(f(x)=ax^2+bx+c(a&gt;0)\\) 为例，任选 \\(x_0\\) 为起点，更新规则 \\(x_{k+1}=x_k-\\alpha f'(x_k)(0\\leqslant \\alpha&lt;\\frac{1}{2a})\\)，请证明：随着 \\(k\\) 的增大，\\(f(x_k)\\rightarrow f(x^*)\\)，其中 \\(x^*\\) 为 \\(f(x)\\) 取得最小值的点"
  },
  {
    "objectID": "linear_regression.html#动手计算",
    "href": "linear_regression.html#动手计算",
    "title": "0x01 线性回归",
    "section": "动手计算",
    "text": "动手计算\n\n损失函数 \\(L(k;T)\\) 推导： \\[\\begin{align}\nL(k;T)&=\\frac{1}{m}((f(x_1)-y_1)^2+(f(x_2)-y_2)^2+...+(f(x_m)-y_m)^2)\\\\&=\\frac{1}{m}\\sum\\limits_{i=1}^m(f(x_i)-y_i)^2=\\frac{1}{m}\\sum\\limits_{i=1}^m(kx_i-y_i)^2\\\\&=\\frac{1}{m}\\sum\\limits_{i=1}^m(k^2x_i^2-2kx_iy_i+y_i^2)\\\\&=\\frac{1}{m}((\\sum\\limits_{i=1}^mx_i^2)k^2+(-2\\sum\\limits_{i=1}^mx_iy_i)k+\\sum\\limits_{i=1}^my_i^2)\n\\end{align}\n\\]"
  },
  {
    "objectID": "linear_regression.html#动手计算-1",
    "href": "linear_regression.html#动手计算-1",
    "title": "0x01 线性回归",
    "section": "动手计算",
    "text": "动手计算\n\n\\(L(k;T)=\\frac{1}{m}((\\sum\\limits_{i=1}^mx_i^2)k^2+(-2\\sum\\limits_{i=1}^mx_iy_i)k+\\sum\\limits_{i=1}^my_i^2)\\)\n\\(L'(k;T)=\\frac{1}{m}(2(\\sum\\limits_{i=1}^mx_i^2)k-2\\sum\\limits_{i=1}^mx_iy_i)\\)，\\(k^*=\\frac{\\sum_{i=1}^mx_iy_i}{\\sum_{i=1}^mx_i^2}\\)\n方法一：手动计算（只要有足够的时间和耐心）\n方法二：Excel\n\n借助公式辅助手动计算（略）\n调用公式；使用散点图\n\n方法三：python\n\n编程模拟计算过程：解析法和导数法；梯度下降法\n调用现成的工具（如sklearn）"
  },
  {
    "objectID": "linear_regression.html#动手计算excel直接调用公式法",
    "href": "linear_regression.html#动手计算excel直接调用公式法",
    "title": "0x01 线性回归",
    "section": "动手计算——Excel直接调用公式法",
    "text": "动手计算——Excel直接调用公式法\n\n在数据旁边选择一个空单元格，输入=LINEST(B2:B28,A2:A28,FALSE)\n\n=表示此单元格内容为公式\nLINEST表示求解线性回归参数的函数\nB2:B28处是 \\(y\\) 值的范围\nA2:A28处是 \\(x\\) 值的范围\nFALSE表示强制截距 \\(b\\) 为 \\(0\\)"
  },
  {
    "objectID": "linear_regression.html#动手计算excel散点图添加趋势线法",
    "href": "linear_regression.html#动手计算excel散点图添加趋势线法",
    "title": "0x01 线性回归",
    "section": "动手计算——Excel散点图添加趋势线法",
    "text": "动手计算——Excel散点图添加趋势线法\n\n选中所有数据（包括列标题）\n点击“插入”——“散点图”\n点击图中任意一点（所有点将被选中）\n右键选择“添加趋势线”\n在右侧弹出的“设置趋势线格式”中勾选“设置截距”和“显示公式”"
  },
  {
    "objectID": "linear_regression.html#python实现梯度下降",
    "href": "linear_regression.html#python实现梯度下降",
    "title": "0x01 线性回归",
    "section": "python实现梯度下降",
    "text": "python实现梯度下降\n\n以计算 \\(f(x)=\\frac{1}{4}(30k^2-56k+30)\\) 的最小值为例\n代码中#及之后的内容是注释，不会被运行\na=b是一个动作，可以不严谨地理解为把b代表的内容“传递”给a，即 \\(a\\leftarrow b\\)\n\ndef f(x): # 定义f(x)的计算方法\n    return (30*k*k-56*k+30)/4\n\ndef ff(x): # 定义f'(x)的计算方法\n    return (60*k-56)/4\n\nx=-2 # x0=-2\n\nwhile True: # 重复以下过程\n    x_=x-0.04*ff(x) # 计算下一个点，alpha=0.04\n    if abs(f(x_)-f(x))&lt;1e-10: # 如果两个点的函数值很接近(差的绝对值小于10^(-10))，则停止\n        break\n    x=x_ # 更新当前点\n\nprint(\"the best x and f(x):\",x,f(x)) # 输出最后的x,f(x)"
  },
  {
    "objectID": "linear_regression.html#python实现线性回归",
    "href": "linear_regression.html#python实现线性回归",
    "title": "0x01 线性回归",
    "section": "python实现线性回归",
    "text": "python实现线性回归\n\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf=pd.read_csv(\"house.csv\")  # house.csv是数据源\nx=df[[\"area\"]] # area指明x列标题\ny=df[\"price\"] # price指明y列标题\nmodel=LinearRegression(fit_intercept=False) # 调用线性回归模型，且设置截距b=0\nmodel.fit(x,y) # 用把数据传递模型\nk=model.coef_ # 获取斜率结果\nprint(\"斜率\",model.coef_[0])\n\nimport numpy as np\ndef f(x):\n    return k*x\nfig=plt.figure(figsize=(15,9))\nax = fig.add_subplot(111)\nx=np.arange(min(x[\"area\"]),max(x[\"area\"]),0.01)\ny=f(x)\nax.plot(x,y)\nx=list(df[\"area\"])\ny=list(df[\"price\"])\nax.scatter(x,y)\nplt.show()\n\n\n\n斜率 10.114737008812858"
  },
  {
    "objectID": "linear_regression.html#配方法",
    "href": "linear_regression.html#配方法",
    "title": "0x01 线性回归",
    "section": "配方法",
    "text": "配方法\n\\[\\begin{align}\nf(x,y)&=\\frac{1}{4}(30x^2+4y^2+20xy-56x-20y+30)\\\\&=\\frac{1}{4}((5x+2y-5)^2+5x^2-6x+5)\n\\end{align}\n\\]\n\n当 \\(x=\\frac{6}{10}\\) 时，\\(5x^2-6x+5\\) 取得最小值，\\(y=1\\) 时，\\((5x+2y-5)^2\\) 可取到 \\(0\\)\n因此最小值点为 \\((\\frac{3}{5},1)\\)，最小值为 \\(\\frac{4}{5}\\)"
  },
  {
    "objectID": "linear_regression.html#练习",
    "href": "linear_regression.html#练习",
    "title": "0x01 线性回归",
    "section": "练习",
    "text": "练习\n\n用配方法求 \\(f(x,y)=\\frac{1}{2} x^{2}+\\frac{1}{4} y^{2}+\\frac{1}{4}x y-2 x-y+3\\) 的最小值"
  },
  {
    "objectID": "linear_regression.html#导数法",
    "href": "linear_regression.html#导数法",
    "title": "0x01 线性回归",
    "section": "导数法",
    "text": "导数法\n认识 \\(f(x,y)=\\frac{1}{4}(30x^2+4y^2+20xy-56x-20y+30)\\)图像"
  },
  {
    "objectID": "linear_regression.html#导数法-1",
    "href": "linear_regression.html#导数法-1",
    "title": "0x01 线性回归",
    "section": "导数法",
    "text": "导数法"
  },
  {
    "objectID": "linear_regression.html#导数法-2",
    "href": "linear_regression.html#导数法-2",
    "title": "0x01 线性回归",
    "section": "导数法",
    "text": "导数法\n\n在已知 \\(y\\) 的情况下，\\(f(x,y)\\) 可以看作以 \\(x\\) 为自变量的一元二次函数 \\(f(x;y)\\)，因此 \\(f(x;y)\\) 有关于 \\(x\\) 的导数（或在 \\(x\\) 方向的切线），记为 \\(f'(x;y)\\)（或 \\(f_x(x,y)\\)、\\(\\frac{\\partial f}{\\partial x}\\)）\n在已知 \\(x\\) 的情况下，类似地，有\\(f'(y;x)\\)（或 \\(f_y(x,y)\\)、\\(\\frac{\\partial f}{\\partial y}\\)）\n对于 \\(f(x,y)=\\frac{1}{4}(30 x^2+ 4 y^2+ 20xy- 56x - 20y  +30   )\\)\n\n\\(f_x(x,y)=\\frac{1}{4}(60x+20y-56)\\)\n\\(f_y(x,y)=\\frac{1}{4}(8y+20x-20)\\)\n令 \\(f_x(x,y)=0\\)且\\(f_y(x,y)=0\\)，得到 \\(x=\\frac{3}{5},y=1\\)"
  },
  {
    "objectID": "linear_regression.html#练习-1",
    "href": "linear_regression.html#练习-1",
    "title": "0x01 线性回归",
    "section": "练习",
    "text": "练习\n用导数法求 \\(f(x,y)=\\frac{1}{2} x^{2}+\\frac{1}{4} y^{2}+\\frac{1}{4}x y-2 x-y+3\\) 的最小值"
  },
  {
    "objectID": "linear_regression.html#梯度下降法-2",
    "href": "linear_regression.html#梯度下降法-2",
    "title": "0x01 线性回归",
    "section": "梯度下降法",
    "text": "梯度下降法\n\n\n\n\n\n\n\n\n\n\n\n\n\n切线指方向，导数定步长\n任选 \\((x_0,y_0)\\) ，每次同时更新： \\[\n\\begin{cases}\nx_{k+1}=x_k-\\alpha f_x(x_k,y_k)\\\\\ny_{k+1}=y_k-\\alpha f_y(x_k,y_k)\n\\end{cases}\n\\]\n可以证明，双参数线性回归损失函数在选取合适的 \\(\\alpha\\) 时，可以收敛到最小值\n更一般地，二元二次函数\\(f(x,y)\\)满足一定的条件（凸性、可微与Lipschitz连续性）时，梯度下降可以收敛最小值"
  },
  {
    "objectID": "linear_regression.html#动手计算-2",
    "href": "linear_regression.html#动手计算-2",
    "title": "0x01 线性回归",
    "section": "动手计算",
    "text": "动手计算\n\n\\(L(k,b;T) =  (\\sum_{i=1}^{m} x_i^2)k^2 + mb^2+ 2( \\sum_{i=1}^{m} x_i)k b - 2 (\\sum_{i=1}^{m}x_i y_i)k- 2 (\\sum_{i=1}^{m} y_i)b  +\\sum_{i=1}^{m} y_i^2\\)\n记 \\(A=\\sum x_i^2,B=\\sum x_i,C=\\sum y_i,D=\\sum x_iy_i,E=\\sum y_i^2,\\bar{x}=\\frac{B}{m}=\\frac{\\sum x_i}{m},\\bar{y}=\\frac{C}{m}=\\frac{\\sum y_i}{m}\\)\n\\(L(k,b;T) =Ak^2+mb^2+2Bkb-2Dk-2Cb+E\\)，先对 \\(b\\) 项配方，再对 \\(k\\) 项配方，得\n\n\\(L(k,b;T)=m(b+\\frac{Bk-C}{m})^2+(\\frac{mA-B^2}{m})(k+\\frac{BC-Dm}{mA-B^2})^2+\\text{常数项}\\)\n\\(k^*=\\frac{Dm-BC}{mA-B^2}=\\frac{\\sum x_iy_i-m\\bar{x}\\bar{y}}{\\sum x_i^2-m\\bar{x}^2},b^*=\\bar{y}-\\bar{x}k\\)\n\n\\(L_k(k,b;T)=2Ak+2Bb-2D=2(Ak+Bb-D)\\)\n\\(L_b(k,b;T)=2mb+2Bk-2C=2(Bk+mb-C)\\)"
  },
  {
    "objectID": "linear_regression.html#动手计算-3",
    "href": "linear_regression.html#动手计算-3",
    "title": "0x01 线性回归",
    "section": "动手计算",
    "text": "动手计算\n\n方法一：手动计算（只要有足够的时间和耐心）\n方法二：Excel\n\n借助公式辅助手动计算（略）\n调用公式（FALSE改为TRUE）；使用散点图（不设置截距）\n\n方法三：python\n\n编程模拟计算过程：解析法和导数法；梯度下降法\n调用现成的工具（如sklearn，fit_intercept=True）"
  },
  {
    "objectID": "linear_regression.html#python实现梯度下降法",
    "href": "linear_regression.html#python实现梯度下降法",
    "title": "0x01 线性回归",
    "section": "python实现梯度下降法",
    "text": "python实现梯度下降法\n以计算 \\(f(x,y)=\\frac{1}{4}(30x^2+4y^2+20xy-56x-20y+30)\\) 最小值为例\nimport numpy as np\ndef f(x,y):\n    return (30*x**2 +4* y**2+20*x*y-56*x-20*y+30)/4\ndef g(x,y): # f在x方向的偏导数\n    return (60*x+20*y-56)/4\ndef h(x,y): # f在y方向的偏导数\n    return (8*y+20*x-20)/4\n\nalpha=0.005 # 学习率\nx,y=1,4\nwhile True:\n    x_,y_=x-alpha*g(x,y),y-alpha*h(x,y)\n    if abs(f(x,y)-f(x_,y_))&lt;1e-7:\n        break\n    x,y=x_,y_\n\nprint(\"the best x,y and f(x,y):\",x,y,f(x,y)) # 输出最后的x,f(x)"
  }
]